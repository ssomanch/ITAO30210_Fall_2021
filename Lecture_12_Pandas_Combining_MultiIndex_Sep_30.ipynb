{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values (contd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mark_data = pd.read_csv('./data/marketing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mark_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* What percentage of missing values for each column in the dataset? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Which attribute has the most missing values in the dataset? (**Hint**: To get the index of the maximum element you can use [`idxmax()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.idxmax.html) function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* How do you fill the missing values with a `0`? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Most Common Use**: Can you fill each missing value with the corresponding average for that attribute?\n",
    "    * For example, if 'Education' attribute is missing for a person, can you find the average 'Education' of all people and fill that missing 'Age' with that average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Pandas Datasets with Concatenation [MORE INFO](https://pandas.pydata.org/pandas-docs/stable/merging.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this tutorial, we will need college_loan_defaults dataset.\n",
    "college_loan_defaults = pd.read_csv(\n",
    "    './data/college-loan-default-rates.csv', index_col='opeid')\n",
    "\n",
    "# Keep in mind that the original dataset has this many rows\n",
    "len(college_loan_defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Office of Postsecondary Education Identification (OPEID) code for each college is used as an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_loan_defaults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pd.concat`\n",
    "You can think of the `pd.concat` function as the equivalent of the NumPy `concatenate` function for `Series` and `DataFrame` objects.\n",
    "\n",
    "Will we spend most of our time on how these function works with `DataFrame` objects as opposed to `Series` objects since in practice that is how it is used most frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to using the `pd.concat` function, the most basic question is whether you are adding *additional rows* or *additional columns*. We'll run through the function arguments based on concatenating rows and then come back for a look at how we perform column concatentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating `DataFrame` Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, I'll split the college_loan_defaults into multiple \n",
    "# sections of rows that we will then stiched back together.\n",
    "part_1 = college_loan_defaults.iloc[:1000]\n",
    "part_2 = college_loan_defaults.iloc[1000:2000]\n",
    "part_3 = college_loan_defaults.iloc[1999:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates three parts:\n",
    "# rows 0-999\n",
    "# rows 1000-1999\n",
    "# rows 1999-end -> notice 1999 appears twice\n",
    "part_3.index & part_2.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all three parts together pd.concat\n",
    "concatenated_dataframe = pd.concat([part_3, part_1, part_2])\n",
    "concatenated_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (concatenated_dataframe.shape[0])\n",
    "print (part_1.shape[0], part_2.shape[0], part_3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Notice that `pd.concat` does not sort the elements of the DataFrame that it returns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duplicate Index Values with `verify_integrity` & `ignore_index` Parameters\n",
    "You probably didn't notice, but we got a school that is appearing twice in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `DataFrame.index.duplicated` function returns a boolean array\n",
    "# we can use as a mask to extract duplicate records.\n",
    "concatenated_dataframe[concatenated_dataframe.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dataframe.loc[1698]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: See how concatenated_dataframe.loc[1698] is using the explit index OPEID to select column(s). Since, we have two rows with the same index (opeid) in the concatenated_dataframe, we get two rows displayed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I purposefully caused this problem for us (by including the 1999 indexed element in both `part_2` and `part_3`; but in the real world this is pretty common!\n",
    "\n",
    "Sometimes you might want to keep both entries (often the case if the index value is the same but the rest of the data is different). If so, you can pass the **`ignore_index`** parameter with a value of **`True`** to the function and **all existing index values will be destroyed** and a new one integer based one will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dataframe = pd.concat([part_2, part_3, part_1], ignore_index=True)\n",
    "concatenated_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If on the other hand, a duplicate index would mean there is a data problem that you don't want to allow, you can specify the `verify_integrity` parameter as `True`.\n",
    "\n",
    "When this is passed, the existence of duplicate indices will generate a `ValueError` exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dataframe = pd.concat([part_2, part_3, part_1], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Column Mismatches with the `join` Parameter\n",
    "Sometimes you will have two sets of rows that you want to join together, but the sets don't have all of the same columns.\n",
    "\n",
    "I'll create a couple of additional small `DataFrame` objects from our college loan dataset to demonstrate our options here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame 1\n",
    "# Contains the first 5 rows of the original dataset\n",
    "# But only the name, city, and state columns\n",
    "name_city_state_columns_only = college_loan_defaults[['name', 'city', 'state']].iloc[:5]\n",
    "\n",
    "# DataFrame 2\n",
    "# Contains the second 5 rows of the original dataset\n",
    "# But only the name, state, and zipcode columns\n",
    "name_state_zipcode_columns_only = college_loan_defaults[['name', 'state', 'zipcode']].iloc[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_city_state_columns_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_state_zipcode_columns_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have have 2 sets of 5 rows that we want to concatenate together, but they have different columns. Let's see what happens if you don't specify anything with the **`join`** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([name_city_state_columns_only, name_state_zipcode_columns_only])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how Pandas adds the special `NaN` value for any column that didn't have a value in the original dataframes? \n",
    "\n",
    "The other option is to drop any columns where there is not data in both sets of rows. You can do this be specifying a value of **`inner`** to the join parameter of the function.\n",
    "\n",
    "Let's demonstrate how doing so will result in only the shared columns (name, state) appearing in the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([name_city_state_columns_only, name_state_zipcode_columns_only], join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating `DataFrame` Columns\n",
    "Now let's go back and see how we can use the `pd.concat` function to merge two sets of columns with the same index (row) values.\n",
    "\n",
    "The data will start out a little dirty but we will clean it up with our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame 1\n",
    "# Contains the first 5 rows of the original dataset\n",
    "# But only the name, city, and state columns\n",
    "name_city_state_columns = college_loan_defaults[['name', 'city', 'state']].iloc[:5]\n",
    "\n",
    "# DataFrame 2\n",
    "# Contains the 7 rows of the original dataset - this will cause a duplicate index\n",
    "# But only default rates columns\n",
    "default_rates = college_loan_defaults[\n",
    "    ['year_1_default_rate',\n",
    "     'year_2_default_rate', \n",
    "     'year_3_default_rate']].iloc[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_city_state_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a simple concatenation. To add columns we have to specify the `axis` parameter with a value of **`1`** or **`col`** to indicate we are adding colums, not rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [name_city_state_columns, default_rates], \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p>\n",
    "Note that the ``pd.concat()`` function is smart to match the rows based on the index `opeid`.  \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of important things to notice here:\n",
    "* Unlike when concatenating rows, this time Pandas did **sort the rows based on the index**. Just something to be aware of.\n",
    "* See how there are a couple of rows with `NaN` values for their first three colums.  That's because our `name_and_default_rates` dataframe had two additional rows for which there were no corresponding values in `name_city_state_zipcode_columns`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the rows with `NaN` values by specifying an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [name_city_state_columns, default_rates], \n",
    "    axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's talk about the how the **`verify_integrity`** and **`ignore_index`** parameters would work when concatenating columns.\n",
    "\n",
    "Let's say that we had included the city column in both dataframes:\n",
    "* The default behavior of `pd.concat` would have been to create a new dataframe with 2 \"city\" columns.\n",
    "* You could make Pandas throw a `ValueError` exception by passing `verify_integrity=True` to the function.\n",
    "* You could also throw out all the column names and replace them with an 0-based series of integers.  This would result in the values of \"city\" being duplicated in two columns, but the columns would have different integer \"names\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets with Merge [MORE INFO](https://pandas.pydata.org/pandas-docs/stable/merging.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be exploring another way to combine datasets through the **`pd.merge`** function.\n",
    "\n",
    "Those who have a background in databases will find a significant amount of overlap between your SQL work and the merge function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 3 Categories of Joins\n",
    "There are 3 different categories of merges/joins which are defined by the characteristics of the shared columns/indices:\n",
    "* One-to-One: Each shared value exists only once in both dataframes.\n",
    "* One-to-Many: A given shared value exists once in first dataframe, but 1 or more times in the second dateframe.\n",
    "* Many-to-Many: A given shared value exists 1 or more times in both dataframes.\n",
    "\n",
    "Let's provide an example of each type of join from our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-One Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p>\n",
    "This will feel pretty similar to concatenating columns.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team Members Favorite Restaurants\n",
    "team_restaurants = pd.DataFrame(\n",
    "    {'restaurant': ['In-N-Out', 'Chipotle', 'Chick-Fil-A'], \n",
    "    'name': ['Mike', 'Kim', 'Roger']})\n",
    "team_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team Members Favorite Restaurants\n",
    "items_locations = pd.DataFrame(\n",
    "    {'items': ['Fries', 'Pizza', 'Barritos','Pasta', 'Shakes'], \n",
    "    'locations': ['Chicago', 'New York', 'San Diego', 'Pittsburgh', 'Seattle']})\n",
    "items_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaurant Items\n",
    "restaurant_items = pd.DataFrame(\n",
    "    {\n",
    "        'item': [\n",
    "        'Shakes', \n",
    "        'Burritos', \n",
    "        'Burger'\n",
    "        ]\n",
    "    ,\n",
    "        'restaurant':[\n",
    "        'In-N-Out',\n",
    "        'Chipotle',\n",
    "        'Five Guys',\n",
    "    ]\n",
    "    }\n",
    ")\n",
    "restaurant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`restaurant`** field in the restuarant_items, team_restaurants dataset is a unique field, that is it the restaurant names appear only once in each dataset. \n",
    "\n",
    "Because of this, if we merge the two dataframes it will be a **1-1 join.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(team_restaurants, restaurant_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Here's what Pandas did:\n",
    "1. Identified the matching column(s) between the two dataframes: **`restaurant`**.\n",
    "1. Found matching **`restaurant`** values between the two dataframes.\n",
    "1. Merged the columns of matching **`restuarant`** values together.\n",
    "1. **Important**: Notice that a new index was generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p>\n",
    "In our discussion, we will reference to the columns that pandas is using to find matches between dataframes as the \"join column(s)\".\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling the Join Type with the `how` Parameter\n",
    "Did you notice that some of the records from each of the original dataframes didn't make it into the merge product?\n",
    "\n",
    "This is because the type of join that was applied to the dataframes was called an **inner join**.\n",
    "\n",
    "The are actually 4 types of joins that you can use:\n",
    "* **Inner Join**: To be included in the output dataframe, the join column(s) value must exist in both original dataframes. \n",
    "    * This is why some of the records didn't get included in the output, because they didn't have a corresponding join column(s) values in the other dataframe.\n",
    "* **Outer Join**: All records from both dataframes are included in the output. Pandas simply fills in `NaN` where there is no corresponding join column(s) value.\n",
    "* **Left Join**: All rows from the first (left) dataframe will be included in the output dataframe, regardless of whether there is a matching join column(s) value in the second (right) dataframe.\n",
    "* **Right Join**: All rows from the second (right) dataframe will be included in the output dataframe, regardless of whether there is a matching join columns value in the left (first) dataframe.\n",
    "\n",
    "Let's go ahead and try all these different types of joins to see how our output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restaurant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outer Join\n",
    "# All records from both dataframes are included.\n",
    "# NaN is inserted into missing grid point.\n",
    "pd.merge(team_restaurants, restaurant_items, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge(team_restaurants, restaurant_items, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge(team_restaurants, restaurant_items, how=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-Many Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Restaurant Items\n",
    "restaurant_items = pd.DataFrame(\n",
    "    {\n",
    "        'item': [\n",
    "        'Burgers', 'Fries', 'Shakes', \n",
    "        'Tacos', 'Burritos', 'Chips',\n",
    "        'Chicken Sandwich', 'Fries', 'Salads'\n",
    "        ]\n",
    "    ,\n",
    "        'rest':[\n",
    "        'In-N-Out', 'In-N-Out', 'In-N-Out', \n",
    "        'Chipotle', 'Chipotle', 'Chipotle',\n",
    "        'Five Guys', 'Five Guys', 'Five Guys'\n",
    "    ]\n",
    "    }\n",
    ")\n",
    "restaurant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge(team_restaurants, restaurant_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the Join Columns\n",
    "Well... that isn't want we wanted.\n",
    "\n",
    "Thankfully though, the error message is pretty self-explanatory. Pandas thinks there are no common columns to merge on.\n",
    "\n",
    "The reason for this is that the common values are held in columns with slightly different names. We have to explain to Pandas what to do when this happens by specifying the names of the columns to join on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restaurant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the left_on and right_on parameters to specify the\n",
    "# name(s) of the join column(s) in the first(left)\n",
    "# and second(right) dataframes.\n",
    "pd.merge(\n",
    "    team_restaurants, \n",
    "    restaurant_items,\n",
    "    left_on='restaurant',\n",
    "    right_on='rest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h5>There can be more than 1 join column</h5>\n",
    "<p>\n",
    "In this example, we have specified only one join column. But you can specify multiple columns if you so desire. Just pass them as a list to the `left_on` and `right_on` parameters.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-Many Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Team Members Favorite Restaurants\n",
    "team_restaurants = pd.DataFrame(\n",
    "    {'restaurant': ['In-N-Out', 'Chipotle', 'Chick-Fil-A', 'Chick-Fil-A', 'In-N-Out'], \n",
    "    'name': ['Mike', 'Kim', 'Roger', 'Sam', 'Sonia']})\n",
    "team_restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restaurant Items\n",
    "restaurant_items = pd.DataFrame(\n",
    "    {\n",
    "        'item': [\n",
    "        'Burgers', 'Fries', 'Shakes', \n",
    "        'Tacos', 'Burritos', 'Chips',\n",
    "        'Chicken Sandwich', 'Fries', 'Salads'\n",
    "        ]\n",
    "    ,\n",
    "        'rest':[\n",
    "        'In-N-Out', 'In-N-Out', 'In-N-Out', \n",
    "        'Chipotle', 'Chipotle', 'Chipotle',\n",
    "        'Five Guys', 'Five Guys', 'Five Guys'\n",
    "    ]\n",
    "    }\n",
    ")\n",
    "restaurant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge(team_restaurants, restaurant_items, left_on = 'restaurant', right_on = 'rest', how = \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p> You could merge two dataframes based on index as well. </p>\n",
    "\n",
    "<p>\n",
    "If you wanted to, you could actually use the index of one dataframe and a column of the other dataframe if you wanted. Pandas gives you great flexibility here. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Compute the population density of each state\n",
    "\n",
    "We will learn ``pd.merge()`` operation using the three datasets from your textbook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop = pd.read_csv('./data/state-population.csv')\n",
    "areas = pd.read_csv('./data/state-areas.csv')\n",
    "abbrevs = pd.read_csv('./data/state-abbrevs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abbrevs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a DataFrame named `areas_abbrvs_merged` by merging the ``areas`` DataFrame and ``abbrevs`` DataFrame to get the state names, state abbreviations, and area into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge the DataFrame created above (`areas_abbrvs_merged`) with the `pop` DataFrame to create a the `desired_data` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, create a column called density using the `population` and `area(sq. mi)` columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which state has highest 'total' population density in year 2012? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter out the data with ages is 'total' and year is 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Find the maximum pop density in the filtered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Find the state that has the maximum pop density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing files back \n",
    "\n",
    "Until now you have been loading the dataset from your computer, however, you might want to store the data back to the computer to use it later. \n",
    "\n",
    "For example, in the above activity were you created population density by merging bunch of DataFrames, you might want to save that DataFrame, rather than redoing all the steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pd.DataFrame.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desired_data.to_csv('./data/state-population-density.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p>\n",
    "``to_csv()`` by default writes the index (row names) as well. This will create an additional column with the indexes. If you want to avoid it, you can use keyword parameter ``index = False`` to avoid creating a column for the index. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desired_data.to_csv('./data/state-population-density.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Indexing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiindex\n",
    "\n",
    "If you set an index to more than one columnn you are creating multi index or Hieararchical index. This makes asking questions based on indexes a lot more easier, and also opens the possibility of working with multidimensional data. \n",
    "\n",
    "We'll use the example sourced from [here](https://chrisalbon.com/python/pandas_hierarchical_data.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], \n",
    "        'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'], \n",
    "        'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n",
    "        'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}\n",
    "df = pd.DataFrame(raw_data, columns = ['regiment', 'company', 'name', 'preTestScore', 'postTestScore'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1_ind = df.set_index('regiment')\n",
    "df_1_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do we get the average scores, based on the regiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1_ind.mean(level = 'regiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How about you want to get the mean scores, based on the company but not the regiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the hierarchical index to be by regiment, and then by company\n",
    "df_2_ind = df.set_index(['regiment', 'company'])\n",
    "df_2_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p>\n",
    "Having multiple indexes will give you an easy way to model more than two dimensional data with DataFrames, which are by default a two dimensional data structures. \n",
    "</p>\n",
    "<p>\n",
    "For the above example, you can imagine each regiment is a two-dimensional array giving details about the company, names and the scores, and they are stacked one below the other. \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2_ind.mean(level='company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2_ind.mean(level='regiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2_ind.mean(level=['regiment','company'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating two or more `Series` to a `DataFrame` with `axis = 1` parameter in `pd.concat()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')\n",
    "s2 = pd.Series([3, 4,5], index=['A', 'B','C'], name='s2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([s1, s2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p>\n",
    "Note that the ``pd.concat()`` function is smart to label the column names with the series variable names. \n",
    "</div> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
