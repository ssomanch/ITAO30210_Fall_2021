{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game of Thrones Dataset\n",
    "\n",
    "In this section, we will use the dataset based on popular book series (and HBO TV series) from George RR Martin, Game of Thrones. The dataset was made available through [Kaggle](https://www.kaggle.com/mylesoneill/game-of-thrones/data) which has information on the character deaths. The dataset was cleaned and we will be working with a sample dataset for this analysis. \n",
    "\n",
    "Game of Thrones is known for abruptly ending its characters. We will use machine learning methods to predict if a character will be alive or dead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "got_data = pd.read_csv(\"./data/GoT_Character_Deaths.csv\")\n",
    "print(got_data.shape)\n",
    "got_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data also includes the 'Name' of the person and the 'Allegiances'. We will remove 'Name' as the name itself is not indicative if the character will alive or dead. We will also remove 'Allegiances' **for now as we do not know how to handle categorical datatype**. In the next class we will handle categorical datatype.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_data.drop(['Name', 'Allegiances'], axis = 1, inplace=True)\n",
    "got_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classfication using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split the input features and outcome variable\n",
    "\n",
    "got_data_X = got_data.drop('dead',1)\n",
    "got_data_Y = got_data['dead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_data_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_test_split()`: Method to split the data into train and test\n",
    "\n",
    "We usually split the data into training set to learn a classifier and then a test set to validate how good our model is \n",
    "\n",
    "Important parameters to this method\n",
    "\n",
    "* **random_state**: Seed to used by randomizer to randomly split the data\n",
    "* **train_size**: Use float to specify what fraction to use for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "got_train_X, got_test_X, got_train_Y, got_test_Y = train_test_split(got_data_X, got_data_Y, random_state=42, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(got_data_X), len(got_train_X), len(got_test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn a classifier: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_regression_model = LogisticRegression()\n",
    "\n",
    "log_regression_model.fit(got_train_X, got_train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "got_predict_Y = log_regression_model.predict(got_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sklmetrics\n",
    "\n",
    "sklmetrics.accuracy_score(got_test_Y, got_predict_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and plotting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = sklmetrics.confusion_matrix(got_test_Y, got_predict_Y, labels =[0,1])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(conf_mat, square=True, annot=True, cbar = False, xticklabels = ['Alive','Dead'], yticklabels = ['Alive','Dead'])\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the feature importance of the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a function to plot coefficients as feature importance\n",
    "# INPUT: Used for Logistic Regression Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most Coefficients\n",
    "def plot_feature_importance_coeff(model, Xnames, cls_nm = None):\n",
    "\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.coef_.ravel())), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance_coeff(log_regression_model, got_data_X.columns, cls_nm=\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dec_tree_model = DecisionTreeClassifier()\n",
    "\n",
    "dec_tree_model.fit(got_train_X, got_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_predict_Y = dec_tree_model.predict(got_test_X)\n",
    "\n",
    "print(sklmetrics.accuracy_score(got_test_Y, got_predict_Y))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(got_test_Y, got_predict_Y, labels =[0,1])\n",
    "print(conf_mat)\n",
    "\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cbar = False, xticklabels = ['Alive','Dead'], yticklabels = ['Alive','Dead'])\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the feature importance of the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a function to plot feature importance for trees\n",
    "# INPUT: Used for Tree based Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most features\n",
    "\n",
    "def plot_feature_importance(model, Xnames, cls_nm = None):\n",
    "\n",
    "    # Measuring important features\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.feature_importances_)), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(dec_tree_model, got_data_X.columns, cls_nm='Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "We will be using the dataset available from [UCI data repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#), that provides information on the phone campaign run by the bank to see if a customer can be converted to have term deposit at their bank. We will only be using a sample from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bank_data = pd.read_csv('./data/bank_campaign_small.csv')\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Data Preprocessing Step - Remove categorical input variables\n",
    "\n",
    "Note that there are some categorical (the data type is object) and the classifiers do not like that datatype. So, we will remove it for now. Later, we will learn how to handle categorical input variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Classification using Logistic Regression and Decision Trees\n",
    "\n",
    "Follow these steps\n",
    "1. Seperate X (input features) and Y (outcome)\n",
    "2. Split into training data and test data. Use 70% of data for training\n",
    "    * Verify if the data is appropriately split by checking the number of rows in each of the training and test data. \n",
    "3. Learn the following two classifiers to predict success or failure\n",
    "    * Logistic Regression\n",
    "    * Decision Tree \n",
    "4. Predict using the test data for both the classifiers\n",
    "5. Provide accuracy score as well as plot the confusion matrix\n",
    "    * Think about the consequence of False Positives and False Negatives\n",
    "6. Provide the variable importance for each classifier\n",
    "    * Use `plot_feature_importance_coeff` for Logistic Regression\n",
    "    * Use `plot_feature_importance` for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
