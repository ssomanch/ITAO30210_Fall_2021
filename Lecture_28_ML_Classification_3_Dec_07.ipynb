{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Classification with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as sklmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank Campaign Dataset\n",
    "\n",
    "We will be using the dataset available from [UCI data repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#), that provides information on the phone campaign run by the bank to see if a customer can be converted to have term deposit at their bank. We will only be using a sample from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv('./data/bank_campaign_small.csv')\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorical Input Variables using `pd.get_dummies()` method\n",
    "\n",
    "In the previous class, we have deleted the categorical input variables, but you don't have to always delete them. You can convert the cateogrical input variables using `get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data_with_dummies = pd.get_dummies(bank_data)\n",
    "bank_data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the additional variables introduced by `pd.get_dummies()` method\n",
    "\n",
    "After removing the additional columns for each categorical variable, we can use this data with dummy columns added as input to various classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data_with_dummies.drop(['marital_divorced','education_unknown','contact_unknown'], axis = 1, inplace=True)\n",
    "bank_data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h5>DO NOT SELECT THE PARAMETERS BASED ON TEST DATA </h5>\n",
    "<p> We have to rather use cross validation techniques discussed below</p>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cross Validation to Select the Parameters of a Model using `GridSearchCV()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, selecting the model parameters has to be done in a cross validation manner. Refer to lecture notes (pdf, pptx) in Sakai for more understanding on cross validation. \n",
    "\n",
    "Scikit-Learn provides a wide variety of cross validation techniques, but the most common way is using `GridSearchCV()` method.\n",
    "\n",
    "**GridSearchCV()** is the process of searching through every possible combination of parameters and selecting the best parameter combination. For example\n",
    "\n",
    "```python\n",
    "params = {'max_depth':[2,3],\n",
    "          'max_features':['auto','log2',None]}\n",
    "```\n",
    "\n",
    "Then you have 6 different combination of parameters listed below. Hence, its called grid search. \n",
    "* max_depth = 2, max_features = 'auto'\n",
    "* max_depth = 3, max_features = 'auto'\n",
    "* max_depth = 2, max_features = 'log2'\n",
    "* max_depth = 3, max_features = 'log2'\n",
    "* max_depth = 2, max_features = None\n",
    "* max_depth = 3, max_features = None\n",
    "\n",
    "** Finally you can select the best model, based on a metric (usually accuracy).** \n",
    "\n",
    "\n",
    "`GridSearchCV()` is very useful method that automates this process of selecting the model with the best set of parameters. The method has four main parameters\n",
    "\n",
    "* **estimator**: The classifier you want to learn the parameters, LogisticRegression, DecisionTreeClassifier, etc. \n",
    "* **param_grid**: Dictionary (dict) of parameters and their values to be searched over. \n",
    "* **cv**: How many folds of classification you want to use. Usually 3 for smaller data and 10 for large data. \n",
    "* **n_jobs**: Usually you specify this as 1. You can parallelize the process of this search by specifying a value more than 1. **Do not have the n_jobs set to more than 3**, for the first time users. Especially, on a laptop or lab machine or on Vocareum, you will end up stalling the machine and it has to be rebooted. For more experienced students, in the class, check the number of processing cores of the machine before you increase the number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Split the input variables (X) and outcome variable (Y)\n",
    "\n",
    "bank_dummies_X = bank_data_with_dummies.drop(['success'], axis = 1)\n",
    "bank_dummies_Y = bank_data_with_dummies['success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Split into training and test data\n",
    "bank_dum_train_X, bank_dum_test_X, bank_dum_train_Y, bank_dum_test_Y = train_test_split(bank_dummies_X, bank_dummies_Y, \n",
    "                                                                                       random_state=37,\n",
    "                                                                                       train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For decision tree based classification\n",
    "\n",
    "\n",
    "# The model you want to set the parameters for\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# The parameters to search over for the model\n",
    "params = {'max_depth':[2,3,4],\n",
    "          'max_features':['auto','log2',None]}\n",
    "\n",
    "\n",
    "# Prepare the GridSearch for cross validation\n",
    "grid_search_dec_tree = GridSearchCV(model, # Note the model is DecisionTreeClassifier as stated above\n",
    "                                    param_grid=params, # The parameters to search over. \n",
    "                                   cv=10, # How many hold out sets to use\n",
    "                                   n_jobs = 1 # Number of parallel processes to run. \n",
    "                                   )\n",
    "\n",
    "# Do the cross validation on the training data \n",
    "grid_search_dec_tree.fit(bank_dum_train_X, bank_dum_train_Y)\n",
    "\n",
    "# Select the best model\n",
    "\n",
    "best_dec_tree_cv = grid_search_dec_tree.best_estimator_\n",
    "\n",
    "# Print the best parameter combination \n",
    "print(grid_search_dec_tree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Notes**\n",
    "1. Usually for large datasets, the above `GridSearchCV` method takes a lot of time. You might have to make sure, you run with limited set of parameters, before you increase the number of possible values for the parameters. \n",
    "2. Everytime you run the GridSearchCV, you might find a different combination of parameters to be the best one. This is another issue with **consistency** of machine learning algorithms. Addressing this is out of the scope for this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally test the performance of the best model on the test data\n",
    "\n",
    "bank_dum_pred_Y = best_dec_tree_cv.predict(bank_dum_test_X)\n",
    "\n",
    "#Print the accuracy \n",
    "print(sklmetrics.accuracy_score(bank_dum_test_Y, bank_dum_pred_Y))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(bank_dum_test_Y, bank_dum_pred_Y)\n",
    "print(conf_mat)\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(conf_mat, fmt='d',square=True, annot=True, cbar = False, xticklabels = ['Failure','Success'], \n",
    "                                                            yticklabels = ['Failure','Success'])\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a function to plot feature importance for trees\n",
    "# INPUT: Used for Tree based Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most features\n",
    "\n",
    "def plot_feature_importance(model, Xnames, cls_nm = None):\n",
    "\n",
    "    # Measuring important features\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.feature_importances_)), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    imp_features = imp_features.iloc[10:]\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(best_dec_tree_cv, bank_dum_train_X.columns, cls_nm='Best CV Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "Learn the parameters of LogisticRegression classifier using `GridSearchCV()` method. \n",
    "1. Specify the dictionary of the parameters for LogisticRegression and the values \n",
    "2. Set GridSearchCV for LogisticRegression classifier. \n",
    "    * Set cv=10\n",
    "3. Use the training data and fit the GridSearchCV so that it learns the model and the best parameter\n",
    "4. Select the best model\n",
    "5. Verify the accuracy and confusion matrix on the testing data. \n",
    "6. Present the importance of each characteristic for Logistic Regression using the `plot_feature_importance_coeff` method below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a function to plot coefficients as feature importance\n",
    "# INPUT: Used for Logistic Regression Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most Coefficients\n",
    "def plot_feature_importance_coeff(model, Xnames, cls_nm = None):\n",
    "\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.coef_.ravel())), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 0\n",
    "model = LogisticRegression(class_weight='balanced', solver = 'liblinear', random_state = 42)\n",
    "\n",
    "# Step 1: Specify the dictionary of the parameters \n",
    "params = {'penalty':['l1','l2'],\n",
    "          'C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Step 2: Prepare the GridSearch for cross validation\n",
    "\n",
    "\n",
    "# Step 3: Do the cross validation on the training data \n",
    "\n",
    "# Step 4: Select the best model\n",
    "\n",
    "\n",
    "# Step 4.1: Print the best parameter combination \n",
    "print(grid_search_log_reg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally test the performance of the best model on the test data\n",
    "\n",
    "\n",
    "\n",
    "#Print the accuracy \n",
    "\n",
    "\n",
    "\n",
    "# Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importance_coeff(best_log_reg_cv, bank_dummies_X.columns, cls_nm='Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for evaluating the performance of classification\n",
    "\n",
    "Until now, we have been always using `accuracy_score` to verif the performance of the classification on the test data. However, in reality this is not usually the most optimal metric. There are other important metrics to use in real-world, shown below. Again, the discussion on these metrics is out of the scope for this course. \n",
    "\n",
    "1. Precision, Recall, F1 Score\n",
    "2. Area Under the Curve (AUC) of Receiver Operating Characteristic (ROC) curve\n",
    "3. Area under Precision-Recall Curve\n",
    "4. Specificity and Sensitivity\n",
    "5. Positive predictive value\n",
    "6. ... many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Many more classification techniques\n",
    "\n",
    "We discussed two basic models in this section. However, there are many other classifiers you can use to improve your classification accuracy. Below, I have provided three main classification methods (by no means they are exhaustive)\n",
    "\n",
    "* MLPClassifier: Multi Layer Perceptron model. This is a very basic model that is a primer to deep learning neural networks. \n",
    "    ```python\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    ```\n",
    "* GradientBoostClassifier: Learns multiple trees to select the best classifier. \n",
    "    ```python\n",
    "    from sklearn.ensemble import GradientBoostClassifier\n",
    "    ```\n",
    "* RandomForestClassifier: Learns multiple trees to select the best classifier. \n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
