{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Classification with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as sklmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank Campaign Dataset\n",
    "\n",
    "We will be using the dataset available from [UCI data repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#), that provides information on the phone campaign run by the bank to see if a customer can be converted to have term deposit at their bank. We will only be using a sample from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv('./data/bank_campaign_small.csv')\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bank_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bank_data['success'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Class Imbalance problem with `class_weight='balanced'`\n",
    "\n",
    "Notice below that in the above bank campaign dataset there are more failures than successes. In these case it is important to let the classifier know that it needs to handle the class imbalance problem. There are many ways to handle the class imbalance problem including oversampling and under sampling, changing the cost of making False Negatives and False Positives. \n",
    "\n",
    "We can do that by creating a classifier with the parameter `class_weight='balanced'`. In that way, the classifier handles the class imbalance problem by choosing the appropriate cost of making False Negatives and False Positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.drop(['marital','education','contact'], axis=1, inplace=True)\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bank_data_X = bank_data.drop('success', axis = 1)\n",
    "bank_data_Y = bank_data['success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bank_train_X, bank_test_X, bank_train_Y, bank_test_Y = train_test_split(bank_data_X, bank_data_Y, random_state = 42, \n",
    "                                                                        train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_logistic = LogisticRegression(random_state=42)\n",
    "\n",
    "bank_logistic.fit(bank_train_X, bank_train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h5>Side Note: Optimization in Logistic Regression</h5>\n",
    "<p>\n",
    "Logistic Regression involves optimization to learn the coefficients of the model. Sometimes those optimizations may not converge. You can change the solvers to see if those can be converged. For example, you can change `solver='lbfgs'` (default) to `solver='liblinear'`\n",
    "</p>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_logistic = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "bank_logistic.fit(bank_train_X, bank_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_predict_Y = bank_logistic.predict(bank_test_X)\n",
    "print(\"The accuracy is {0}\".format(sklmetrics.accuracy_score(bank_test_Y, bank_predict_Y)))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(bank_test_Y, bank_predict_Y, labels =[0,1])\n",
    "print(conf_mat)\n",
    "\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cbar = False, xticklabels = ['Failure','Success'], \n",
    "            yticklabels = ['Failure','Success'], fmt = 'g')\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_logistic_balanced = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced')\n",
    "\n",
    "bank_logistic_balanced.fit(bank_train_X, bank_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_predict_Y = bank_logistic_balanced.predict(bank_test_X)\n",
    "print(\"The accuracy is {0}\".format(sklmetrics.accuracy_score(bank_test_Y, bank_predict_Y)))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(bank_test_Y, bank_predict_Y, labels =[0,1])\n",
    "print(conf_mat)\n",
    "\n",
    "sns.heatmap(conf_mat, square=True, annot=True, cbar = False, xticklabels = ['Failure','Success'], \n",
    "            yticklabels = ['Failure','Success'], fmt = 'g')\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorical Input Variables using `pd.get_dummies()` method\n",
    "\n",
    "In the previous class, we have deleted the categorical input variables, but you don't have to always delete them. You can convert the cateogrical input variables using `get_dummies()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h5>Remove the one category for each categorical variable</h5>\n",
    "<p>\n",
    "As you will see the `get_dummies` method introducing a column for each value of the cateogorical varible. It is **very important** to remove one column for each categorical variable for the models to appropriately work. \n",
    "</p>\n",
    "<p>\n",
    "For example, in for the bank data, for the categorical variable marital status, may be we can remove say 'marital_divorced', and for education may be we can remove 'education_unknown' and for contact we can remove 'contact_unknown'. \n",
    "</p>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv('./data/bank_campaign_small.csv')\n",
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data['marital'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data['education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data['contact'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data_with_dummies = pd.get_dummies(bank_data)\n",
    "bank_data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the additional variables introduced by `pd.get_dummies()` method\n",
    "\n",
    "After removing the additional columns for each categorical variable, we can use this data with dummy columns added as input to various classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bank_data_with_dummies.drop(['marital_divorced','education_unknown','contact_unknown'], axis = 1, inplace=True)\n",
    "bank_data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also use the keyword argument `drop_first` to remove the first value for each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data_with_dummies_auto = pd.get_dummies(bank_data, drop_first=True)\n",
    "bank_data_with_dummies_auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of the Classification models\n",
    "\n",
    "Every classification model has set of parameters that need to be set before the model can be learned. In the folllowing we explain a couple of important parameters for Logistic Regression and Decision Tree Classification\n",
    "\n",
    "* **Logistic Regression**: When you are running logistic regression, which is very similar to multiple linear regression, having a lot of input variables can lead to overfit in the data. Hence, hence there number of ways to reduce the number of input variables used in the logistic regression. These methods are called **variable selection** methods. One classic machine learning method to do the variable selection with the data is through **regularization**. You can think of regularization as selecting only a subset of input features that can be used in the logistic regression. Rather than doing manually, regularization is a statistically rigorous way of doing the **variable selection**. There are two parameters with respect to regularization in LogisticRegression classifier. The main idea of these parameters is to **penalize models the use more of input variables** in the logistic regression\n",
    "    * **penalty**: This is a kind of penalty you wish to apply. The most common ones are **['l1','l2']**. \n",
    "    * **C**: The weight that needs to be given for regularization. The more the value of C, the more preference for using more input variables. The smaller the value of C, the stronger the regularization and hence less input variables is preferred. The most common values are from **[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]**\n",
    "    \n",
    "    \n",
    "* **Decision Tree**: Recollect decision tree is building a tree with increasing depth. The two important parameters to set are \n",
    "    * **max_depth**: How deep should the decision tree be built. Usually, depends on the data and **most common values are [3,4,5]**\n",
    "    * **max_features**: How many features should be considered when splitting a decision tree node. **The most common values are ['auto','log2', None]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "Create LogisticRegression and DecisionTreeClassifier for various parameter choices. **Which parameter combination has best accuracy score for**? Select the model parameter that has good accuracy on the test data. \n",
    "\n",
    "**Important Note**: This way of selecting the model parameters is strongly recommended against. **YOU SHOULD NEVER DO THIS**. **This is only for learning**. Look below cross-validation technique to select the right model parameters. \n",
    "\n",
    "\n",
    "An example of to create a model with certain parameters is shown below. \n",
    "\n",
    "```python\n",
    "# Logistic regression model with penalty = 'l1' and the weight of penalization C = 1\n",
    "log_reg_with_parameter = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced', penalty='l1', C= 0.1)\n",
    "```\n",
    "\n",
    "Use `bank_data_with_dummies` as your starting dataset. Remember to do all the steps of the classification and choose the model that has the highest accuracy. Remember the outcome variable of interest is 'success'. \n",
    "\n",
    "First step is done for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Split the input variables (X) and outcome variable (Y)\n",
    "\n",
    "bank_dummies_X = bank_data_with_dummies.drop(['success'], axis = 1)\n",
    "bank_dummies_Y = bank_data_with_dummies['success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Split into training and test data.  Use 70% of the data for training.\n",
    "bank_dum_train_X, bank_dum_test_X, bank_dum_train_Y, bank_dum_test_Y = train_test_split(bank_dummies_X, bank_dummies_Y, \n",
    "                                                                                       random_state=42,\n",
    "                                                                                       train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3a: Learn the classifier with various parameters on the training data for Logistic Regression.  \n",
    "# Try these combinations\n",
    "\n",
    "# penalty = 'l1', C = 0.1\n",
    "# penalty = 'l1', C = 1\n",
    "# penalty = 'l1', C = 10\n",
    "# penalty = 'l2', C = 0.1\n",
    "# penalty = 'l2', C = 0.1\n",
    "# penalty = 'l2', C = 10\n",
    "log_reg_with_parameter = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced', penalty='l1', C= 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Use the test data to predict the Y and then print the accuracy_score for Logistic Regression \n",
    "\n",
    "#Get an accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3b: Learn the classifier with various parameters on the training data for Decision Tree. \n",
    "# Try these combinations\n",
    "\n",
    "# max_depth = 2, max_features = 'auto'\n",
    "# max_depth = 3, max_features = 'auto'\n",
    "# max_depth = 2, max_features = 'log2'\n",
    "# max_depth = 3, max_features = 'log2'\n",
    "# max_depth = 2, max_features = None\n",
    "# max_depth = 3, max_features = None\n",
    "\n",
    "dec_tree_with_parameter = DecisionTreeClassifier(random_state=42, class_weight='balanced', max_depth = 2, max_features = 'auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Use the test data to predict the Y and then print the accuracy_score for Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h5>DO NOT SELECT THE PARAMETERS BASED ON TEST DATA </h5>\n",
    "<p> We have to rather use cross validation techniques discussed below</p>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cross Validation to Select the Parameters of a Model using `GridSearchCV()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, selecting the model parameters has to be done in a cross validation manner. Refer to lecture notes (pdf, pptx) in Sakai for more understanding on cross validation. \n",
    "\n",
    "Scikit-Learn provides a wide variety of cross validation techniques, but the most common way is using `GridSearchCV()` method.\n",
    "\n",
    "**GridSearchCV()** is the process of searching through every possible combination of parameters and selecting the best parameter combination. For example\n",
    "\n",
    "```python\n",
    "params = {'max_depth':[2,3],\n",
    "          'max_features':['auto','log2',None]}\n",
    "```\n",
    "\n",
    "Then you have 6 different combination of parameters listed below. Hence, its called grid search. \n",
    "* max_depth = 2, max_features = 'auto'\n",
    "* max_depth = 3, max_features = 'auto'\n",
    "* max_depth = 2, max_features = 'log2'\n",
    "* max_depth = 3, max_features = 'log2'\n",
    "* max_depth = 2, max_features = None\n",
    "* max_depth = 3, max_features = None\n",
    "\n",
    "** Finally you can select the best model, based on a metric (usually accuracy).** \n",
    "\n",
    "\n",
    "`GridSearchCV()` is very useful method that automates this process of selecting the model with the best set of parameters. The method has four main parameters\n",
    "\n",
    "* **estimator**: The classifier you want to learn the parameters, LogisticRegression, DecisionTreeClassifier, etc. \n",
    "* **param_grid**: Dictionary (dict) of parameters and their values to be searched over. \n",
    "* **cv**: How many folds of classification you want to use. Usually 3 for smaller data and 10 for large data. \n",
    "* **n_jobs**: Usually you specify this as 1. You can parallelize the process of this search by specifying a value more than 1. **Do not have the n_jobs set to more than 3**, for the first time users. Especially, on a laptop or lab machine or on Vocareum, you will end up stalling the machine and it has to be rebooted. For more experienced students, in the class, check the number of processing cores of the machine before you increase the number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For decision tree based classification\n",
    "\n",
    "# The model you want to set the parameters for\n",
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# The parameters to search over for the model\n",
    "params = {'max_depth':[2,3,4],\n",
    "          'max_features':['auto','log2',None]}\n",
    "\n",
    "\n",
    "# Prepare the GridSearch for cross validation\n",
    "grid_search_dec_tree = GridSearchCV(model, # Note the model is DecisionTreeClassifier as stated above\n",
    "                                    param_grid=params, # The parameters to search over. \n",
    "                                   cv=3, # How many hold out sets to use\n",
    "                                   n_jobs = 1 # Number of parallel processes to run.  \n",
    "                                   )\n",
    "\n",
    "# Do the cross validation on the training data \n",
    "grid_search_dec_tree.fit(bank_dum_train_X, bank_dum_train_Y)\n",
    "\n",
    "# Select the best model\n",
    "\n",
    "best_dec_tree_cv = grid_search_dec_tree.best_estimator_\n",
    "\n",
    "# Print the best parameter combination \n",
    "print(grid_search_dec_tree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Notes**\n",
    "1. Usually for large datasets, the above `GridSearchCV` method takes a lot of time. You might have to make sure, you run with limited set of parameters, before you increase the number of possible values for the parameters. \n",
    "2. Everytime you run the GridSearchCV, you might find a different combination of parameters to be the best one. This is another issue with **consistency** of machine learning algorithms. Addressing this is out of the scope for this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally test the performance of the best model on the test data\n",
    "\n",
    "bank_dum_pred_Y = best_dec_tree_cv.predict(bank_dum_test_X)\n",
    "\n",
    "#Print the accuracy \n",
    "print(sklmetrics.accuracy_score(bank_dum_test_Y, bank_dum_pred_Y))\n",
    "\n",
    "conf_mat = sklmetrics.confusion_matrix(bank_dum_test_Y, bank_dum_pred_Y)\n",
    "print(conf_mat)\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(conf_mat, fmt='g',square=True, annot=True, cbar = False, xticklabels = ['Failure','Success'], \n",
    "                                                            yticklabels = ['Failure','Success'])\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.ylabel(\"True Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the feature importance of the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a function to plot feature importance for trees\n",
    "# INPUT: Used for Tree based Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most features\n",
    "\n",
    "def plot_feature_importance(model, Xnames, cls_nm = None):\n",
    "\n",
    "    # Measuring important features\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.feature_importances_)), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    imp_features = imp_features.iloc[10:]\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(best_dec_tree_cv, bank_dum_train_X.columns, cls_nm='Best CV Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "Learn the parameters of LogisticRegression classifier using `GridSearchCV()` method. \n",
    "1. Specify the dictionary of the parameters for LogisticRegression and the values \n",
    "2. Set GridSearchCV for LogisticRegression classifier. \n",
    "    * Set cv=10\n",
    "3. Use the training data and fit the GridSearchCV so that it learns the model and the best parameter\n",
    "4. Select the best model\n",
    "5. Verify the accuracy and confusion matrix on the testing data. \n",
    "6. Present the importance of each characteristic for Logistic Regression using the `plot_feature_importance_coeff` method below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the feature importance of the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a function to plot coefficients as feature importance\n",
    "# INPUT: Used for Logistic Regression Classifier\n",
    "#        Feature Names\n",
    "# OUTPUT: A plot of top most Coefficients\n",
    "def plot_feature_importance_coeff(model, Xnames, cls_nm = None):\n",
    "\n",
    "    imp_features = pd.DataFrame(np.column_stack((Xnames, model.coef_.ravel())), columns = ['feature', 'importance'])\n",
    "    imp_features[['importance']] = imp_features[['importance']].astype(float)\n",
    "    imp_features[['abs_importance']] = imp_features[['importance']].abs()\n",
    "    # Sort the features based on absolute value of importance\n",
    "    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(cls_nm + \" - Feature Importance\")\n",
    "    plt.barh(range(imp_features.shape[0]), imp_features['importance'],\n",
    "            color=\"b\", align=\"center\")\n",
    "    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )\n",
    "    plt.ylim([-1, imp_features.shape[0]])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(cls_nm + \"_feature_imp.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for evaluating the performance of classification\n",
    "\n",
    "Until now, we have been always using `accuracy_score` to verif the performance of the classification on the test data. However, in reality this is not usually the most optimal metric. There are other important metrics to use in real-world, shown below. Again, the discussion on these metrics is out of the scope for this course. \n",
    "\n",
    "1. Precision, Recall, F1 Score\n",
    "2. Area Under the Curve (AUC) of Receiver Operating Characteristic (ROC) curve\n",
    "3. Area under Precision-Recall Curve\n",
    "4. Specificity and Sensitivity\n",
    "5. Positive predictive value\n",
    "6. ... many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Many more classification techniques\n",
    "\n",
    "We discussed two basic models in this section. However, there are many other classifiers you can use to improve your classification accuracy. Below, I have provided three main classification methods (by no means they are exhaustive)\n",
    "\n",
    "* MLPClassifier: Multi Layer Perceptron model. This is a very basic model that is a primer to deep learning neural networks. \n",
    "    ```python\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    ```\n",
    "* GradientBoostClassifier: Learns multiple trees to select the best classifier. \n",
    "    ```python\n",
    "    from sklearn.ensemble import GradientBoostClassifier\n",
    "    ```\n",
    "* RandomForestClassifier: Learns multiple trees to select the best classifier. \n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
